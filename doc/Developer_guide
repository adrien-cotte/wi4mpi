############################# Wi4MPI License ###########################
# `04/04/2016`                                                         #
# Copyright or (C) or Copr. Commissariat a l'Energie Atomique          #
#                                                                      #
# IDDN.FR.001.210022.000.S.P.2014.000.10800                            #
# This file is part of the Wi4MPI library.                             #
#                                                                      #
# SPDX-License-Identifier: BSD-3-Clause OR CeCILL-B                    #
#                                                                      #
#                                                                      #
# Authors:                                                             #
#   - Delforge Tony <tony.delforge.tgcc@cea.fr>                        #
#   - Ducrot Vincent <vincent.ducrot.tgcc@cea.fr>                      #
#   - Cotte Adrien <adrien.cotte.tgcc@cea.fr>                          #
#   - Juilly Kevin <kevin.juilly.tgcc@cea.fr>                          #
#                                                                      #
########################################################################


:Authors:
    Tony Delforge (tony.delforge.tgcc@cea.fr)
    Vincent Ducrot (vincent.ducrot.tgcc@cea.fr)
    Adrien Cotte (adrien.cotte.tgcc@cea.fr)
    Juilly Kevin (kevin.juilly.tgcc@cea.fr)

Developer Guide
===============

Sommaire:
---------
1. Introduction
2. Library
    1. How it works
    2. Implémentation
        1. Library settings
        2. Symbol overload
        3. Code chooser ASM
        4. A_MPI_Function
        5. R_MPI_Function
        6. Hashtable
        7. Thread safety
        8. Jit handling user function
        9. The interface
    3. Files
3. Get Involved in WI4MPI

------------

INTRODUCTION
============
MPI is a standard in HPC community which allows a simple use of clusters. Nowoaday, there are several implementation (OpenMPI, BullxMPI, MPT, IntelMPI, MPC, ...) each of which involves a specific ABI (Application Binary Interface) for an application compiled with a specific MPI implementation.
With wi4mpi, an application compiled with an alpha MPI implementation can be run under a beta MPI implementation without any compilation protocol and any concern about the ABI (Preload version).
WI4MPI can also be seen as a dedicated MPI implementation; This time, the application is compiled against the wi4mpi library (libmpi.so) whith the dedicated wrapper (mpicc,mpif90...) meant for that purpose, and can be run under any other MPI implementation (Interface Version).

Library
=======

How it works
------------

Before performing any translation we need to distinguish the application side from the runtime side. To do that, any MPI object from the application side are prefixed by A_ and those from the application are prefixed by R_.
To perform a translation, all original MPI call from the application is intercepted by WI4MPI and remplaced by the same call prefixed by A_. 
For example, with an OpenMPI ---> IntelMPI conversion:


Application: MPI_Init (OpenMPI)                         MPI_Init (IntelMPI)
                              \                         /
                      Phase 1  \                       /  Phase 3
                                \                     /
                                |--------------------|
                                | WI4MPI: A_MPI_Init |
                                |--------------------|
                                          |
                                          | Phase 2 
                                          | 
                                      Translation

Implementation
--------------

Library settings
~~~~~~~~~~~~~~~~

The library is set during its loading time, when the program start. All runtime MPI routines are saved in function pointer via dlsym call to make phase 3 possible, all the table are created and set with MPI constant object, and the spinlocks are initialized. To do so, we used the following syntax:

void __attribute__((constructor)) wrapper_init
{

    void (*)lib_handle=dlopen(getenv("WI4MPI_RUN_MPI_C_LIB"),RTLD_NOW); 
    LOCAL_MPI_Function=dlsym(lib_handle,"PMPI_Function") 
    ....
}

The library contain three constructor:

- wrapper_init dans test_generation_wrapper.c (API C) (preload and interface)
- wrapper_init_f dans wrapper.c (API Fortran) (preload and interface)
- wrapper_init_c2ff2c dans c2f_f2c.c (API c2f/f2c) 


Symbol overload
~~~~~~~~~~~~~~~

The mpi called are intercept thanks to the following rerouting:

- #define A_MPI_Send PMPI_Send
- #pragma weak MPI_Send=PMPI_Send

(See interface_test.c in src/interface/gen/interface_test.c and src/interface/gen/interface_fort.c)
This syntax is also present but hidden in test_generation_wrapper.c (src/{preload,interface}/gen/) whithin an asm code chooser for the next reason.

The MPI-IO implémentation (ROMIO) present within the most MPI implementation trigger some call to the MPI user interface that WI4MPI intercept thanks to its symbols overload protocol. It means that during the runtime (phase 3 above), some MPI calls are made, forcing WI4MPI to intercept them and so making the application to crash. The crash is die to the fact that WI4MPI MPI is trying to convert runtime argument to runtime version.

Example:

Application:MPI_File_open (OpenMPI)                           MPI_File_open(IntelMPI)
                             \                               /                      \
              Phase 1         \                             /         Phase 3        \
                               \                           /                          \
                                |-------------------------|                      |----------------------------------------------------|
                                | WI4MPI: A_MPI_File_open |                      | WI4MPI: A_MPI_Allreduce but with runtime arguments |
                                |-------------------------|                      | instead of application arguments (R_ instead of A_)|
                                            |                                    |----------------------------------------------------|
                                            |   Phase 2                                                     |
                                            |                                                               |
                                       Translation                                                     Translation ----> Crash




To overcome this issue, we used an ASM code chooser. 

Code chooser ASM
~~~~~~~~~~~~~~~~

The ASM code chooser does the simple following things:

If we alreday are in the wrapper:

- The arguments are passed without any translation protocol to the underlying MPI runtime call (LOCAL_MPI_function)

Otherwise:

- The arguments are translated and passed to the underlying MPI runtime call (LOCAL_MPI_function)

To know which state the process is, we defined the in_w variable:

- in_w=1 : in the wrapper
- in_w=0 : in the application

Since the implementation of MPI objects are developer dependent, some of these may have different size among the different one. To make sure that there is no side effect, the code chooser analyze the stack itself.

ASM Code chooser implementation (generated for each function):

- .global PMPI_Function 
- .weak MPI_Function 
- .set MPI_function,PMPI_Function
- .extern in_w 
- .extern A_MPI_Function
- .extern R_MPI_Function
- .type PMPI_Function,@function
- .text
- PMPI_Function:
- push %rbp 
- mov %rsp, %rbp 
- sub $0x20, %rsp 
- mov %rdi, -0x8(%rbp) 
- mov %rsi, -0x10(%rbp) 
- mov %rdx, -0x18(%rbp) 
- mov %rcx, -0x20(%rbp) 
- .byte 0x66 
- leaq in_w@tlsgd(%rip), %rdi 
- .value 0x6666 
- rex64 
- call __tls_get_addr@PLT 
- mov -0x8(%rbp), %rdi 
- mov -0x10(%rbp), %rsi 
- mov -0x18(%rbp), %rdx 
- mov -0x20(%rbp), %rcx 
- leave 
- cmpl $0x0, 0x0(%rax) 
- jne inwrap_MPI_Function
- jmp (*)A_MPI_Function@GOTPCREL(%rip) 
- inwrap_MPI_Function:
- jmp (*)R_MPI_Function@GOTPCREL(%rip) 
- .size PMPI_Function,.-PMPI_Function

Application:MPI_File_open (OpenMPI)                           MPI_File_open(IntelMPI)
                             \                               /                      \
              Phase 1         \                             /         Phase 3        \
                               \                           /                          \
                                |-------------------------|                     |-------------------------|
                                | WI4MPI: PMPI_File_open  |                     | WI4MPI: PMPI_Allreduce  |
                                | Testing in_w: in_w=0    |                     | Testing in_w: in_w=1    |
                                |-------------------------|                     | ------------------------|
                                           |    Phase 2                                      |
                                           |                                                 |
                                A_MPI_File_open:Translation                     R_MPI_Allreduce:No Translation

A_MPI_Function
~~~~~~~~~~~~~~

All translations are executed thanks to some mappers defined within mappers.h using an underlying hash table mechanism named uthash (https://troydhanson.github.io/uthash/)
The mappers (see example below) always have the same syntax : 

 - mapper_name_a2r/r2a(&buf, &buf_tmp);

In case of an a2r translation, buf_tmp represent the translation of buf and vice versa for an r2a translation.

Exemple:

A_MPI_Send(void * buf,int count,A_MPI_Datatype datatype,int dest,int tag,A_MPI_Comm comm)

{

    void * buf_tmp;

    const_buffer_conv_a2r(&buf,&buf_tmp); **mapper**

    R_MPI_Datatype datatype_tmp;

    datatype_conv_a2r(&datatype,&datatype_tmp); **mapper**

    int dest_tmp;

    dest_conv_a2r(&dest,&dest_tmp); **mapper**

    int tag_tmp;

    tag_conv_a2r(&tag,&tag_tmp); **mapper**

    R_MPI_Comm comm_tmp;

    comm_conv_a2r(&comm,&comm_tmp); **mapper**

    int ret_tmp= LOCAL_MPI_Send( buf_tmp, count, datatype_tmp, dest_tmp, tag_tmp, comm_tmp); **Runtime MPI_Send call**

    return error_code_conv_r2a(ret_tmp); 

}


R_MPI_Function
~~~~~~~~~~~~~~
For an R_MPI_Function, the arguments are directly passed to the MPI runtime call

int R_MPI_Send(void * buf,int count,R_MPI_Datatype datatype,int dest,int tag,R_MPI_Comm comm)
{

    int ret_tmp= LOCAL_MPI_Send( buf, count, datatype, dest, tag, comm);

    return ret_tmp;

}

Hash table
~~~~~~~~~~

The underlying hashtable mechanism presented earlier are contained in the new_utils.*, new_utils_fn.* and utash.h.
For each MPI objects, two tables are created. One for the constants, and one for the MPI_Type created bby the application.

The differents type being:

- MPI_Comm
- MPI_Datatype
- MPI_Errhandler
- MPI_Group
- MPI_Op
- MPI_Request **Séparé en 2 tables, afin de dissocier les requêtes persistantes des requêtes non-bloquantes**
- MPI_File

The table within new_utils_fn.* contain the following translation:

- MPI_Handler_function
- MPI_Comm_copy_attr_function
- MPI_Comm_delete_function
- MPI_Type_delete_function
- MPI_Comm_errhandler_function
- MPI_File_errhandler_function 


Thread safety
~~~~~~~~~~~~~
To make WI4MPI usable in a multithread environment, the in_w (see above) variable is TLS protected.

- __thread int in_w=0; (test_wrapper_generation.c:118)
- extern __thread int in_w; (wrapper.c:7)
- extern __thread int in_w; (c2f_f2c.c:6) || (c2f_f2c.c:1149)

The table are spinlock protected. (cf :thread_safety.h):

- #define lock_dest(a) pthread_spin_destroy(a)
- #define lock_init(a) pthread_spin_init(a,PTHREAD_PROCESS_PRIVATE)
- #define lock(a)  pthread_spin_lock(a)
- #define unlock(a) pthread_spin_unlock(a)
- typedef  pthread_spinlock_t (*)table_lock_t;

Interface:
----------

The interface version of WI4MPI propose the promise as the preload version (one compilation, several run over different MPI implementation), but this time WI4MPI had to be seen as a fully MPI Library. 
All the previously section are still relevent for the interface, the only things that changed is the new level name INTERFACE (see the schema below). This level has to be considered as a "libmpi.so" which is linked to the user application.

               
              dlopen|----------|  dlopen       |---------|
                   /| Lib_OMPI | ----------- > | OpenMPI |
                  / |----------|               |---------|
   |-----------| / 
   |           |/
   | INTERFACE |
   | libmpi.so |\
   |-----------| \
                  \
                   \|----------|  dlopen       |----------|
                    | Lib_IMPI | ----------- > | IntelMPI |
              dlopen|----------|               |----------|

The files interface_test.c and interface_fort.c, deal with the overload symbol mechanism see earlier for respectively the C and Fortran API, then according the conversion a dlopen is made to the appropriate library (WI4MPI_WRAPPER_LIB) responsible for the traduction (ASM code chooser + A_MPI_Function + R_MPI_Function).

MPI_Init example:

int MPI_Init(int * argc,char *** argv);
#define MPI_Init PMPI_Init
#pragma weak MPI_Init=PMPI_Init
int (*INTERFACE_LOCAL_MPI_Init)(int *,char ***);

int PMPI_Init(int * argc,char *** argv)
{
int ret_tmp= INTERFACE_LOCAL_MPI_Init( argc, argv);
return ret_tmp;
}
__attribute__((constructor)) void wrapper_interface(void) {
void *interface_handle=dlopen(getenv("WI4MPI_WRAPPER_LIB"),RTLD_NOW|RTLD_GLOBAL);
if(!interface_handle)
{
    printf("no true IC lib defined\nerror :%s\n",dlerror());
    exit(1);
}
INTERFACE_LOCAL_MPI_Init=dlsym(interface_handle,"CCMPI_MPI_Init");
}

Static mode
-----------
The static mode builds an executable with every targets translation. To avoid conflicts, symbols are renamed as follow: INTERF2_{TARGET}_{Symbol_name}.
No more dlopen is needed (cf. Interface), functions pointer are choosen by 2 variables: WI4MPI_STATIC_TARGET_TYPE_F et WI4MPI_STATIC_TARGET_TYPE.
Static sections are controled by directives: #if(n)def WI4MPI_STATIC / #endif
------------

Common files for both version of WI4MPI:

 - func_char_fort.*: 
    Contain all Fortran MPI functions that deal with some character arguments.
    Since in Fortran a character argument always reference is len (character(len=*) :: dark_side) and since the len argument is not the same size according to the compiler (Intel/GNU < 8 or GNU >= 8) used,
    WI4MPI had to implement both.

    Example --
           #ifdef IFORT_CALL
                  void  A_f_MPI_Get_processor_name(char * name,int * resultlen,int * ret,int namelen) **The character length is of type int**
           #elif GFORT_CALL
                  void  A_f_MPI_Get_processor_name(char * name,int * resultlen,int * ret,size_t namelen) **The character length is of type size_t**
           #endif
           --

 - manual_wrapper.h: Contain some manual mappers for Fortran translation
 - mappers.h: Contain the a2r/r2a mappers for C translattion
 - new_utils.*, new_utils_fn.*, and uthash.h: Contain all table routines
 - thread_safety.h: Contain the spinlock protection

Preload files:

- bin/wi4mpi: see User_Guide 
- etc/wi4mpi.cfg: see User_Guide
- gen:
    - c2f_f2c.c:
    - lib_empty.c: Empty file to create empty Libraries made to remplace the one from MPI use for the compilation
    - test_generation_wrapper.c: contain all C MPI function within WI4MPI which deal with the translation
    - wrapper.c: contain all the Fortran MPI function within WI4MPI which deal with the translation
- header:
    - INTEL_INTEL: app_mpi.h app_mpio.h run_mpi.h run_mpio.h wrapper_f.h
    - INTEL_OMPI: app_mpi.h  app_mpio.h run_mpi.h wrapper_f.h
    - OMPI_INTEL: app_mpi.h  run_mpi.h  run_mpio.h wrapper_f.h
    - OMPI_OMPI: app_mpi.h run_mpi.h wrapper_f.h

Interface files:

- gen: 
    - c2f_f2c.c: 
    - test_generation_wrapper.c: Same as the preload version
    - wrapper.c: Same as the preload version
    - interface_fort.c: Contain the overload symbol mechanism for Fortran MPI Function 
    - interface_test.c: Contain the overload symbol mechanism for C MPI Function and rerooting to CodeChooser
- header:
    - OMPI_INTEL: app_mpi.h  run_mpi.h  run_mpio.h wrapper_f.h
    - OMPI_OMPI: pp_mpi.h run_mpi.h wrapper_f.h
- lib_cccmpi:
    - bin: Contain all mpi wrapper for compilation
    - include: Contain all include exposed to users
- manual:
    - dlsym_global.c : Get runtime MPI constantes 
- module: Contain all elements to create a descent module

Get involved in WI4MPI
======================

Generator_guide is prerequisites to this part

Expand MPI cover of WI4MPI
--------------------------

On the generator side :
~~~~~~~~~~~~~~~~~~~~~~~

- Add the function name to the func_list_....txt files
- Add the function description in the dictionary functions.json
- Add the new mappers (if needed) to convert the arguments in the dictionay mappers.json
- Get involved in the generator code if some special case have to be handled
- Generate the new Fortran header for both interface and preload version

On the library side:
~~~~~~~~~~~~~~~~~~~~

- Code the new mappers in mappers.h, new_utils*
- Update app_mpi.h app_mpio.h run_mpi.h run_mpio.h for all conversion of both version
- Update headers within src/interface/lib_cccmpi/include
- Make sure to respect the MPI norme

Expand WI4MPI conversion capability
-----------------------------------

- In mappers.h, you have to make sure that the status mapper translate the MPI_Status.count in the right way since its implementation is developer dependent.
- Generate the associated app_mpi.h and run_mpi.h to new conversion


